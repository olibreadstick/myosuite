diff --git a/logs/myoHandWheelHoldFixed-v0_0.monitor.csv b/logs/myoHandWheelHoldFixed-v0_0.monitor.csv
index 8b7cb2d..765003e 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_0.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_0.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_1.monitor.csv b/logs/myoHandWheelHoldFixed-v0_1.monitor.csv
index 0accd6d..d25200c 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_1.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_1.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_10.monitor.csv b/logs/myoHandWheelHoldFixed-v0_10.monitor.csv
index 2c518c7..79f4568 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_10.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_10.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_11.monitor.csv b/logs/myoHandWheelHoldFixed-v0_11.monitor.csv
index 0601862..8307130 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_11.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_11.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_2.monitor.csv b/logs/myoHandWheelHoldFixed-v0_2.monitor.csv
index 7a45b91..6e37e39 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_2.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_2.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_3.monitor.csv b/logs/myoHandWheelHoldFixed-v0_3.monitor.csv
index 2f5d146..fb2414f 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_3.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_3.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_4.monitor.csv b/logs/myoHandWheelHoldFixed-v0_4.monitor.csv
index cae2fdc..6fd1a88 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_4.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_4.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_5.monitor.csv b/logs/myoHandWheelHoldFixed-v0_5.monitor.csv
index c550e99..fe8908f 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_5.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_5.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_6.monitor.csv b/logs/myoHandWheelHoldFixed-v0_6.monitor.csv
index 8aa1904..4dcf116 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_6.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_6.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_7.monitor.csv b/logs/myoHandWheelHoldFixed-v0_7.monitor.csv
index dc69e88..47d899b 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_7.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_7.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_8.monitor.csv b/logs/myoHandWheelHoldFixed-v0_8.monitor.csv
index a6c7963..95fc43f 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_8.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_8.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_9.monitor.csv b/logs/myoHandWheelHoldFixed-v0_9.monitor.csv
index 5911160..aab01cf 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_9.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_9.monitor.csv differ
diff --git a/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py b/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py
index ff12bd1..4250773 100644
--- a/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py
+++ b/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py
@@ -174,19 +174,19 @@ if __name__ == "__main__":
 
     print("Begin training")
 
-    model = PPO('MlpPolicy', envs, verbose=1, ent_coef=0.001,
-            policy_kwargs=policy_kwargs,
-            tensorboard_log=f"runs/{time_now}")
+    #model = PPO('MlpPolicy', envs, verbose=1, ent_coef=0.001,
+     #      policy_kwargs=policy_kwargs,
+      #      tensorboard_log=f"runs/{time_now}")
     
     # TODO TRY LOADING
-    # model_num =   '2025_07_23_19_37_27'
-    # model = PPO.load('./MPL_baselines/policy_best_model'+ '/'+ env_name + '/' + model_num + r'/best_model', envs, verbose = 1, ent_coeff = 0.01, policy_kwargs = policy_kwargs, tensorboard_log=f"runs/{time_now}")
+    model_num =   '2025_07_23_19_37_27'
+    model = PPO.load('./MPL_baselines/policy_best_model'+ '/'+ env_name + '/' + model_num + r'/best_model', envs, verbose = 1, ent_coeff = 0.01, policy_kwargs = policy_kwargs, tensorboard_log=f"runs/{time_now}")
 
     obs_callback = TensorboardCallback()
     callback = CallbackList([eval_callback, WandbCallback(gradient_save_freq=100)])#, obs_callback])
 
     #TODO TOTAL TIMESTEPS HERE
-    model.learn(total_timesteps=2e6, tb_log_name=env_name + "_" + time_now, callback=callback)
+    model.learn(total_timesteps=5e5, tb_log_name=env_name + "_" + time_now, callback=callback)
     model.save(curr_dir+'/WheelDist_policy')
 
     # evaluate policy
@@ -207,4 +207,4 @@ if __name__ == "__main__":
     all_rewards
 
     # # Record video after training
-    # record_video(env_name, log_path)
\ No newline at end of file
+    record_video(env_name, log_path)
\ No newline at end of file
diff --git a/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip b/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip
index 74edd8b..39f521f 100644
Binary files a/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip and b/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip differ
diff --git a/myosuite/envs/myo/myowheelchair/render.py b/myosuite/envs/myo/myowheelchair/render.py
index 8d3b8e6..f7aab31 100644
--- a/myosuite/envs/myo/myowheelchair/render.py
+++ b/myosuite/envs/myo/myowheelchair/render.py
@@ -11,7 +11,10 @@ if __name__ == "__main__":
     env.reset()
 
     model = PPO("MlpPolicy", env, verbose=0)
-    pi = PPO.load(r"C:\Users\jasmi\Documents\GitHub\myosuite\MPL_baselines\policy_best_model\myoHandWheelHoldFixed-v0\2025_07_24_02_31_48\best_model.zip")
+    pi = PPO.load(r"/Users/oliviacardillo/myosuite/myosuite3/MPL_baselines/policy_best_model/myoHandWheelHoldFixed-v0/2025_07_24_16_24_10/best_model.zip")
+
+    env = gym.make('myoHandWheelHoldFixed-v0')
+    env.reset()
 
     # render
     frames = []
diff --git a/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4 b/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4
index c02b7a9..2f693e8 100644
Binary files a/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4 and b/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4 differ
diff --git a/myosuite/envs/myo/myowheelchair/wheelhold_v0.py b/myosuite/envs/myo/myowheelchair/wheelhold_v0.py
index 9be8803..db666ac 100644
--- a/myosuite/envs/myo/myowheelchair/wheelhold_v0.py
+++ b/myosuite/envs/myo/myowheelchair/wheelhold_v0.py
@@ -51,6 +51,9 @@ class WheelHoldFixedEnvV0(BaseV0):
             weighted_reward_keys:list = DEFAULT_RWD_KEYS_AND_WEIGHTS,
             **kwargs,
         ):
+        self.time_step = 0
+        self.return_phase_start = 40
+        
         # self.goal_sid_right = self.sim.model.site_name2id("wheelchair_grip_right") #green, movable
         self.palm_r = self.sim.model.site_name2id("palm_r")
         self.hand_start_right = self.sim.model.site_name2id("hand_start_right") #red, static
@@ -151,6 +154,9 @@ class WheelHoldFixedEnvV0(BaseV0):
         return obs_dict
 
     def get_reward_dict(self, obs_dict):
+        return_phase = self.time_step >= self.return_phase_start
+        
+        
         # palm position from green dot (WARNING: MOVES WITH RAIL)
         # dist_right = np.linalg.norm(obs_dict['wheel_err_right']) 
 
@@ -197,6 +203,43 @@ class WheelHoldFixedEnvV0(BaseV0):
         hand_err = np.linalg.norm(obs_dict["hand_err"])
         return_err = np.linalg.norm(obs_dict["return_err"])
 
+        # if return_phase:
+        # # Return phase: only use return_reward
+        #     return_reward = np.exp(-50.0 * return_err) - 0.5 * return_err
+        #     rwd_dict = collections.OrderedDict((
+        #         ('return_rwd', return_reward),
+        #         ('hand_err_rwd', 0),
+        #         ('dist_reward', 0.5*dist_reward),
+        #         ('palm_touch_rwd', 0),
+        #         ('wheel_rotation', 0),
+        #         ('act_reg', -0.25 * act_mag),
+        #         ('fin_open', np.exp(fin_open)),
+        #         ('bonus', 0),
+        #         ('penalty', 0),
+        #         ('sparse', return_err < 0.025),
+        #         ('solved', 0),
+        #         ('done', 0),
+        #     ))
+        #         # ('solved', return_err < 0.0025),
+        #         # ('done', return_err > 50.0),
+        #     print(f"[Return] Step: {self.time_step}, err: {return_err:.3f}, reward: {return_reward:.3f}")
+        # else:
+        #     # Normal phase: compute all rewards
+        #     rwd_dict = collections.OrderedDict((
+        #         ('return_rwd', 0),
+        #         ('hand_err_rwd', math.exp(-20.0 * abs(hand_err))),
+        #         ('dist_reward', dist_reward),
+        #         ('palm_touch_rwd', palm_touch_rwd),
+        #         ('wheel_rotation', 15.0 * wheel_rotation),
+        #         ('act_reg', -0.5 * act_mag),
+        #         ('fin_open', np.exp(-5.0 * fin_open)),
+        #         ('bonus', 1.0 * (wheel_angle_now < self.init_wheel_angle)),
+        #         ('penalty', -1.0 * (wheel_angle_now > self.init_wheel_angle)),
+        #         ('sparse', 0),
+        #         ('solved', 0),
+        #         ('done', 0)
+        #     ))
+
         # === Compute reward based on phase ===
         if self.task_phase == "push":
             rwd_dict = collections.OrderedDict((
@@ -215,13 +258,15 @@ class WheelHoldFixedEnvV0(BaseV0):
                 # ('solved', wheel_rotation < -500.0),
                 # ('done', wheel_rotation > 500.0),
             ))
-            if hand_err < 0.005 and elbow_now < 0.74:
+            if abs(wheel_rotation) >= (np.pi / 18):
+                print("10 deg reached")
                 print("returning")
                 self.task_phase = "return"
 
         elif self.task_phase == "return":
+            return_reward = np.exp(-50.0 * return_err) - 0.5 * return_err
             rwd_dict = collections.OrderedDict((
-                ('return_rwd', math.exp(-20.0 * abs(return_err))),
+                ('return_rwd', return_reward),
                 ('hand_err_rwd', 0),
                 ('dist_reward', 0.5*dist_reward),
                 ('palm_touch_rwd', 0),
@@ -235,6 +280,7 @@ class WheelHoldFixedEnvV0(BaseV0):
                 ('done', 0),
                 # ('solved', return_err < 0.0025),
                 # ('done', return_err > 50.0),
+                print(f"[Return] Step: {self.time_step}, err: {return_err:.3f}, reward: {return_reward:.3f}")
             ))
             if return_err < 0.005 and elbow_now > 1.0:
                 print("return successful, pushing again")
@@ -250,7 +296,11 @@ class WheelHoldFixedEnvV0(BaseV0):
     def reset(self, **kwargs):
         self.robot.sync_sims(self.sim, self.sim_obsd)
         obs = super().reset(**kwargs)
-
+        self.time_step = 0
         # self.prev_rwd_dict = self.get_reward_dict(self.get_obs_dict(self.sim))  # optional init
-        
-        return obs
\ No newline at end of file
+
+        return obs
+
+    def step(self, action):
+        self.time_step += 1
+        return super().step(action)
\ No newline at end of file
diff --git a/myosuite/envs/myo/myowheelchair/wheelhold_v0_hand_return.py b/myosuite/envs/myo/myowheelchair/wheelhold_v0_hand_return.py
deleted file mode 100644
index 8aaebcb..0000000
--- a/myosuite/envs/myo/myowheelchair/wheelhold_v0_hand_return.py
+++ /dev/null
@@ -1,194 +0,0 @@
-""" =================================================
-# Copyright (c) Facebook, Inc. and its affiliates
-Authors  :: Vikash Kumar (vikashplus@gmail.com), Vittorio Caggiano (caggiano@gmail.com)
-================================================= """
-
-## PUSHING WHEELCHAIR ##
-
-# TODO traing from 174109, make fin open 0, make wheel rotation 15???
-
-import collections
-import numpy as np
-import math
-from myosuite.utils import gym
-
-from myosuite.envs.myo.base_v0 import BaseV0
-
-
-class WheelHoldFixedEnvV0(BaseV0):
-
-    DEFAULT_OBS_KEYS = ['time', 'wheel_err_right', 'wheel_angle', 'hand_qpos', 'hand_qvel']
-    DEFAULT_RWD_KEYS_AND_WEIGHTS = {
-        "goal_dist": 0.0,
-        "hand_dist" : 0.0,
-        "fin_open": -10.0,
-        "bonus": 0.0,
-        "penalty": 0,
-        "wheel_rotation": 10.0,
-        "rotation_bonus": 2.0
-    }
-
-    def __init__(self, model_path, obsd_model_path=None, seed=None, **kwargs):
-
-        # EzPickle.__init__(**locals()) is capturing the input dictionary of the init method of this class.
-        # In order to successfully capture all arguments we need to call gym.utils.EzPickle.__init__(**locals())
-        # at the leaf level, when we do inheritance like we do here.
-        # kwargs is needed at the top level to account for injection of __class__ keyword.
-        # Also see: https://github.com/openai/gym/pull/1497
-        gym.utils.EzPickle.__init__(self, model_path, obsd_model_path, seed, **kwargs)
-
-        # This two step construction is required for pickling to work correctly. All arguments to all __init__
-        # calls must be pickle friendly. Things like sim / sim_obsd are NOT pickle friendly. Therefore we
-        # first construct the inheritance chain, which is just __init__ calls all the way down, with env_base
-        # creating the sim / sim_obsd instances. Next we run through "setup"  which relies on sim / sim_obsd
-        # created in __init__ to complete the setup.
-        super().__init__(model_path=model_path, obsd_model_path=obsd_model_path, seed=seed, env_credits=self.MYO_CREDIT)
-
-        self._setup(**kwargs)
-
-
-    def _setup(self,
-            obs_keys:list = DEFAULT_OBS_KEYS,
-            weighted_reward_keys:list = DEFAULT_RWD_KEYS_AND_WEIGHTS,
-            **kwargs,
-        ):
-        self.goal_sid_right = self.sim.model.site_name2id("wheelchair_grip_right")
-        self.palm_r = self.sim.model.site_name2id("palm_r")
-        self.hand_start_right = self.sim.model.site_name2id("hand_start_right")
-        self.rail_bottom_right = self.sim.model.site_name2id("rail_bottom_right")
-
-        # define the palm and tip site id.
-        # self.palm_r = self.sim.model.site_name2id('S_grasp')
-        self.init_palm_z = self.sim.data.site_xpos[self.palm_r][-1]
-        self.fin0 = self.sim.model.site_name2id("THtip")
-        self.fin1 = self.sim.model.site_name2id("IFtip")
-        self.fin2 = self.sim.model.site_name2id("MFtip")
-        self.fin3 = self.sim.model.site_name2id("RFtip")
-        self.fin4 = self.sim.model.site_name2id("LFtip")
-
-        self.wheel_joint_id = self.sim.model.joint_name2id("right_rear")
-        self.init_wheel_angle = self.sim.data.qpos[self.wheel_joint_id]
-
-
-        #self.goal_sid_left = self.sim.model.site_name2id("wheel_grip_goal_left")
-        #self.object_init_pos = self.sim.data.site_xpos[self.object_sid].copy()
-
-        super()._setup(obs_keys=obs_keys,
-                    weighted_reward_keys=weighted_reward_keys,
-                    **kwargs,
-        )
-        
-        self.init_qpos = self.sim.model.key_qpos[0].copy() # copy the sitting + grabbing wheels keyframe
-
-
-    def get_obs_vec(self):
-        self.obs_dict['time'] = np.array([self.sim.data.time])
-        self.obs_dict['hand_qpos'] = self.sim.data.qpos[13:].copy()
-        self.obs_dict['hand_qvel'] = self.sim.data.qvel[12:].copy()*self.dt
-        #self.obs_dict['wheel_pos'] = self.sim.data.site_xpos[self.object_sid]
-        self.obs_dict['wheel_err_right'] = self.sim.data.site_xpos[self.goal_sid_right] - self.sim.data.site_xpos[self.palm_r]
-        self.obs_dict['hand_initpos_err_right'] = self.sim.data.site_xpos[self.hand_start_right]- self.sim.data.site_xpos[self.goal_sid_right]
-
-        self.obs_dict["palm_pos"] = self.sim.data.site_xpos[self.palm_r]
-        self.obs_dict['fin0'] = self.sim.data.site_xpos[self.fin0]
-        self.obs_dict['fin1'] = self.sim.data.site_xpos[self.fin1]
-        self.obs_dict['fin2'] = self.sim.data.site_xpos[self.fin2]
-        self.obs_dict['fin3'] = self.sim.data.site_xpos[self.fin3]
-        self.obs_dict['fin4'] = self.sim.data.site_xpos[self.fin4]
-
-        self.obs_dict["rail_bottom_right"] = self.sim.data.site_xpos[self.rail_bottom_right]
-
-        self.obs_dict['wheel_angle'] = np.array([self.sim.data.qpos[self.wheel_joint_id]])
-
-
-        if self.sim.model.na>0:
-            self.obs_dict['act'] = self.sim.data.act[:].copy()
-
-        t, obs = self.obsdict2obsvec(self.obs_dict, self.obs_keys)
-        return obs
-
-    def get_obs_dict(self, sim):
-        obs_dict = {}
-        obs_dict['time'] = np.array([sim.data.time])
-        obs_dict['hand_qpos'] = sim.data.qpos[13:].copy()
-        obs_dict['hand_qvel'] = sim.data.qvel[12:].copy()*self.dt
-        #obs_dict['wheel_pos'] = sim.data.site_xpos[self.object_sid]
-        #obs_dict['wheelchair_grip_right'] = sim.data.site_xpos[self.goal_sid] - sim.data.site_xpos[self.object_sid]
-        obs_dict['wheel_err_right'] = sim.data.site_xpos[self.goal_sid_right] - sim.data.site_xpos[self.palm_r]
-        obs_dict['hand_initpos_err_right'] = sim.data.site_xpos[self.hand_start_right]- sim.data.site_xpos[self.goal_sid_right]
-        #add the initial and end target points
-        #could add the fingertips here,
-        obs_dict["palm_pos"] = sim.data.site_xpos[self.palm_r]
-        obs_dict['fin0'] = sim.data.site_xpos[self.fin0]
-        obs_dict['fin1'] = sim.data.site_xpos[self.fin1]
-        obs_dict['fin2'] = sim.data.site_xpos[self.fin2]
-        obs_dict['fin3'] = sim.data.site_xpos[self.fin3]
-        obs_dict['fin4'] = sim.data.site_xpos[self.fin4]
-
-        obs_dict["rail_bottom_right"] = sim.data.site_xpos[self.rail_bottom_right]
-
-        obs_dict['wheel_angle'] = np.array([sim.data.qpos[self.wheel_joint_id]])
-
-
-        #obs_dict['wheel_err_left'] = sim.data.site_xpos[self.goal_sid] - sim.data.site_xpos[self.object_sid]
-        if sim.model.na>0:
-            obs_dict['act'] = sim.data.act[:].copy()
-        
-        return obs_dict
-
-    def get_reward_dict(self, obs_dict):
-        dist_right = np.linalg.norm(obs_dict['wheel_err_right'])
-        hand_initpos_err_right = np.linalg.norm(obs_dict['hand_initpos_err_right'])
-
-        #for wheel rotation
-        wheel_angle_now = self.sim.data.qpos[self.wheel_joint_id]
-        wheel_rotation = wheel_angle_now - self.init_wheel_angle
-        wheel_target = np.pi / 2  # 90 degrees
-
-        wheel_rotation_err = abs(wheel_rotation - wheel_target)
-        wheel_rotation_rwd = math.exp(-5.0 * wheel_rotation_err)
-
-        
-        
-        act_mag = np.linalg.norm(self.obs_dict['act'], axis=-1)/self.sim.model.na if self.sim.model.na !=0 else 0
-        drop = dist_right > 0.500
-
-        fin_keys = ['fin0', 'fin1', 'fin2', 'fin3', 'fin4']
-        # for fin in fin_keys:
-        #     print(fin, type(obs_dict[fin]), np.shape(obs_dict[fin]))
-        fin_open = sum(
-            np.linalg.norm(obs_dict[fin].squeeze() - obs_dict['rail_bottom_right'].squeeze(), axis=-1)
-            for fin in fin_keys
-        )
-        
-        # grip_right = self._check_hand_grip_contact(
-        #     hand_geom_names=["right_index_tip", "right_thumb_tip"],
-        #     wheel_geom_names=[f"handrail_coll{i}" for i in range(1, 17)]
-        # )
-
-        rwd_dict = collections.OrderedDict((
-            ('goal_dist', math.exp(-2.0*abs(dist_right))), #exp(- k * abs(x))
-            ('hand_dist', math.exp(-1.0*abs(hand_initpos_err_right))),
-            ('bonus', 1.*(dist_right<2*0) + 1.*(dist_right<0)),
-            ('act_reg', -1.*act_mag),
-            ("fin_open", np.exp(-20 * fin_open)),  # fin_open + np.log(fin_open +1e-8)
-
-            #('grip_bonus', 1.0 * grip_right),
-            ('penalty', -1.*drop),
-            ('sparse', dist_right < 0.055),
-            #('sparse', 1.0 * grip_right - dist_right),
-            ('solved', dist_right < 0.001 and wheel_rotation_err < 0.05),
-            #('solved', grip_right and dist_right < 0.015),
-            ('done', dist_right > 0.9),
-            ('wheel_rotation', wheel_rotation_rwd),
-            ('rotation_bonus', 1.0 if wheel_rotation_err < 0.05 else 0.0),
-        ))
-        
-        rwd_dict['dense'] = np.sum([wt*rwd_dict[key] for key, wt in self.rwd_keys_wt.items()], axis=0)
-        
-        return rwd_dict
-    
-    def reset(self, **kwargs):
-        self.robot.sync_sims(self.sim, self.sim_obsd)
-        obs = super().reset(**kwargs)
-        return obs
diff --git a/myosuite/envs/myo/myowheelchair/wheelhold_v0_push.py b/myosuite/envs/myo/myowheelchair/wheelhold_v0_push.py
index 2688806..e75dd18 100644
--- a/myosuite/envs/myo/myowheelchair/wheelhold_v0_push.py
+++ b/myosuite/envs/myo/myowheelchair/wheelhold_v0_push.py
@@ -143,7 +143,7 @@ class WheelHoldFixedEnvV0(BaseV0):
         #for wheel rotation
         wheel_angle_now = self.sim.data.qpos[self.wheel_joint_id]
         wheel_rotation = wheel_angle_now - self.init_wheel_angle
-        wheel_target = np.pi / 3  # 90 degrees
+        wheel_target = np.pi / 2  # 90 degrees
 
         wheel_rotation_err = abs(wheel_rotation - wheel_target)
         wheel_rotation_rwd = math.exp(-5.0 * wheel_rotation_err)
@@ -192,3 +192,10 @@ class WheelHoldFixedEnvV0(BaseV0):
         self.robot.sync_sims(self.sim, self.sim_obsd)
         obs = super().reset(**kwargs)
         return obs
+    
+    def restore_hand_to_initial_pose(self):
+        """Move the hand back to its initial grip pose (keyframe pose) during an episode."""
+        self.sim.data.qpos[:] = self.init_qpos.copy()
+        self.sim.data.qvel[:] = 0
+        self.sim.forward()
+
diff --git a/myosuite/envs/myo/myowheelchair/wheelhold_v0_return.py b/myosuite/envs/myo/myowheelchair/wheelhold_v0_return.py
index 2e651ad..9bbf991 100644
--- a/myosuite/envs/myo/myowheelchair/wheelhold_v0_return.py
+++ b/myosuite/envs/myo/myowheelchair/wheelhold_v0_return.py
@@ -3,10 +3,6 @@
 Authors  :: Vikash Kumar (vikashplus@gmail.com), Vittorio Caggiano (caggiano@gmail.com)
 ================================================= """
 
-## PUSHING WHEELCHAIR ##
-
-# TODO traing from 174109, make fin open 0, make wheel rotation 15???
-
 import collections
 import numpy as np
 import math
@@ -16,17 +12,19 @@ from myosuite.envs.myo.base_v0 import BaseV0
 
 
 class WheelHoldFixedEnvV0(BaseV0):
-    
-    DEFAULT_OBS_KEYS = ['time', 'wheel_err_right', 'wheel_angle', 'hand_qpos', 'hand_qvel', 'hand_start_right']
+
+    DEFAULT_OBS_KEYS = ['time', 'wheel_angle', 'hand_qpos', 'hand_qvel']
     DEFAULT_RWD_KEYS_AND_WEIGHTS = {
-        "goal_dist": 10.0,
-        "hand_dist" : 0.0,
-        "fin_open": -15.0,
-        "bonus": 0.0,
-        "penalty": 2.0,
-        "wheel_rotation": 15.0,
-        "rotation_bonus": 2.0,
-        "return_reward": 10.0
+        "return_rwd": 30.0,
+        "hand_err_rwd": 10.0,
+
+        "dist_reward": 5.0,
+        "palm_touch_rwd": 5.0,
+        "wheel_rotation": 25.0,
+        "fin_open": -2.0,
+
+        "bonus": 1.0, 
+        "penalty": 1.0,
     }
 
     def __init__(self, model_path, obsd_model_path=None, seed=None, **kwargs):
@@ -53,14 +51,16 @@ class WheelHoldFixedEnvV0(BaseV0):
             weighted_reward_keys:list = DEFAULT_RWD_KEYS_AND_WEIGHTS,
             **kwargs,
         ):
-        
         self.time_step = 0
         self.return_phase_start = 40
         
-        self.goal_sid_right = self.sim.model.site_name2id("wheelchair_grip_right")
+        # self.goal_sid_right = self.sim.model.site_name2id("wheelchair_grip_right") #green, movable
         self.palm_r = self.sim.model.site_name2id("palm_r")
-        self.hand_start_right = self.sim.model.site_name2id("hand_start_right")
-        self.rail_bottom_right = self.sim.model.site_name2id("rail_bottom_right")
+        self.hand_start_right = self.sim.model.site_name2id("hand_start_right") #red, static
+        # self.rail_bottom_right = self.sim.model.site_name2id("rail_bottom_right") #blue, movable
+        
+        # hand target position, start returning when reached
+        self.hand_TARGET_right = self.sim.model.site_name2id("hand_TARGET_right") #blue, STATIC
 
         # define the palm and tip site id.
         # self.palm_r = self.sim.model.site_name2id('S_grasp')
@@ -71,12 +71,15 @@ class WheelHoldFixedEnvV0(BaseV0):
         self.fin3 = self.sim.model.site_name2id("RFtip")
         self.fin4 = self.sim.model.site_name2id("LFtip")
 
-        self.wheel_joint_id = self.sim.model.joint_name2id("right_rear")
-        self.init_wheel_angle = self.sim.data.qpos[self.wheel_joint_id]
+        self.wheel_joint_id = self.sim.model.joint_name2id("right_rear") #right wheel joint
+        self.init_wheel_angle = self.sim.data.qpos[self.wheel_joint_id].copy() #INIITAL wheel angle
 
+        #elbow jnt
+        self.elbow_joint_id = self.sim.model.joint_name2id("elbow_flexion")
 
-        #self.goal_sid_left = self.sim.model.site_name2id("wheel_grip_goal_left")
-        #self.object_init_pos = self.sim.data.site_xpos[self.object_sid].copy()
+        #phases check
+        self.task_phase = "push"
+        # self.prev_rwd_dict = None
 
         super()._setup(obs_keys=obs_keys,
                     weighted_reward_keys=weighted_reward_keys,
@@ -90,9 +93,9 @@ class WheelHoldFixedEnvV0(BaseV0):
         self.obs_dict['time'] = np.array([self.sim.data.time])
         self.obs_dict['hand_qpos'] = self.sim.data.qpos[13:].copy()
         self.obs_dict['hand_qvel'] = self.sim.data.qvel[12:].copy()*self.dt
-        #self.obs_dict['wheel_pos'] = self.sim.data.site_xpos[self.object_sid]
-        self.obs_dict['wheel_err_right'] = self.sim.data.site_xpos[self.goal_sid_right] - self.sim.data.site_xpos[self.palm_r]
-        self.obs_dict['hand_initpos_err_right'] = self.sim.data.site_xpos[self.hand_start_right]- self.sim.data.site_xpos[self.goal_sid_right]
+
+        # self.obs_dict['wheel_err_right'] = self.sim.data.site_xpos[self.goal_sid_right] - self.sim.data.site_xpos[self.palm_r]
+        # self.obs_dict['hand_initpos_err_right'] = self.sim.data.site_xpos[self.hand_start_right]- self.sim.data.site_xpos[self.goal_sid_right]
 
         self.obs_dict["palm_pos"] = self.sim.data.site_xpos[self.palm_r]
         self.obs_dict['fin0'] = self.sim.data.site_xpos[self.fin0]
@@ -101,10 +104,16 @@ class WheelHoldFixedEnvV0(BaseV0):
         self.obs_dict['fin3'] = self.sim.data.site_xpos[self.fin3]
         self.obs_dict['fin4'] = self.sim.data.site_xpos[self.fin4]
 
-        self.obs_dict["rail_bottom_right"] = self.sim.data.site_xpos[self.rail_bottom_right]
+        # self.obs_dict["rail_bottom_right"] = self.sim.data.site_xpos[self.rail_bottom_right]
 
         self.obs_dict['wheel_angle'] = np.array([self.sim.data.qpos[self.wheel_joint_id]])
+        # self.obs_dict["task_phase"] = np.array([1.0 if self.task_phase == "push" else -1.0])
+
+        #calculate palm from target distance
+        self.obs_dict['hand_err'] = self.sim.data.site_xpos[self.palm_r]- self.sim.data.site_xpos[self.hand_TARGET_right]
 
+        #calculate palm to return position
+        self.obs_dict['return_err'] = self.sim.data.site_xpos[self.palm_r]- self.sim.data.site_xpos[self.hand_start_right]
 
         if self.sim.model.na>0:
             self.obs_dict['act'] = self.sim.data.act[:].copy()
@@ -117,13 +126,10 @@ class WheelHoldFixedEnvV0(BaseV0):
         obs_dict['time'] = np.array([sim.data.time])
         obs_dict['hand_qpos'] = sim.data.qpos[13:].copy()
         obs_dict['hand_qvel'] = sim.data.qvel[12:].copy()*self.dt
-        #obs_dict['wheel_pos'] = sim.data.site_xpos[self.object_sid]
-        #obs_dict['wheelchair_grip_right'] = sim.data.site_xpos[self.goal_sid] - sim.data.site_xpos[self.object_sid]
-        obs_dict['wheel_err_right'] = sim.data.site_xpos[self.goal_sid_right] - sim.data.site_xpos[self.palm_r]
-        obs_dict['hand_initpos_err_right'] = sim.data.site_xpos[self.hand_start_right]- sim.data.site_xpos[self.goal_sid_right]
-        #add the initial and end target points
-        #could add the fingertips here,
-        obs_dict['hand_start_right'] = self.sim.data.site_xpos[self.hand_start_right].copy()
+
+        # obs_dict['wheel_err_right'] = sim.data.site_xpos[self.goal_sid_right] - sim.data.site_xpos[self.palm_r]
+        # obs_dict['hand_initpos_err_right'] = sim.data.site_xpos[self.hand_start_right]- sim.data.site_xpos[self.goal_sid_right]
+        
         obs_dict["palm_pos"] = sim.data.site_xpos[self.palm_r]
         obs_dict['fin0'] = sim.data.site_xpos[self.fin0]
         obs_dict['fin1'] = sim.data.site_xpos[self.fin1]
@@ -131,102 +137,170 @@ class WheelHoldFixedEnvV0(BaseV0):
         obs_dict['fin3'] = sim.data.site_xpos[self.fin3]
         obs_dict['fin4'] = sim.data.site_xpos[self.fin4]
 
-        obs_dict["rail_bottom_right"] = sim.data.site_xpos[self.rail_bottom_right]
-
+        # obs_dict["rail_bottom_right"] = sim.data.site_xpos[self.rail_bottom_right]
         obs_dict['wheel_angle'] = np.array([sim.data.qpos[self.wheel_joint_id]])
 
+        # obs_dict["task_phase"] = np.array([1.0 if self.task_phase == "push" else -1.0])
+
+        #calculate palm from target distance
+        obs_dict['hand_err'] = self.sim.data.site_xpos[self.palm_r]- self.sim.data.site_xpos[self.hand_TARGET_right]
+
+        #calculate palm to return position
+        obs_dict['return_err'] = self.sim.data.site_xpos[self.palm_r]- self.sim.data.site_xpos[self.hand_start_right]
 
-        #obs_dict['wheel_err_left'] = sim.data.site_xpos[self.goal_sid] - sim.data.site_xpos[self.object_sid]
         if sim.model.na>0:
             obs_dict['act'] = sim.data.act[:].copy()
         
         return obs_dict
 
     def get_reward_dict(self, obs_dict):
-        
         return_phase = self.time_step >= self.return_phase_start
         
-        dist_right = np.linalg.norm(obs_dict['wheel_err_right'])
-        hand_initpos_err_right = np.linalg.norm(obs_dict['hand_initpos_err_right'])
-
-        #for wheel rotation
+        
+        # palm position from green dot (WARNING: MOVES WITH RAIL)
+        # dist_right = np.linalg.norm(obs_dict['wheel_err_right']) 
+
+        # CHECK IF PALM TOUCHING WHEEL
+        palm_body_id = self.sim.model.body_name2id("thirdmc")
+        rail_body_id = self.sim.model.body_name2id("right_handrail")
+        palm_touching_rail = False
+        for i in range(self.sim.data.ncon):
+            con = self.sim.data.contact[i]
+            body1 = self.sim.model.geom_bodyid[con.geom1]
+            body2 = self.sim.model.geom_bodyid[con.geom2]
+            if ((body1 == palm_body_id and body2 == rail_body_id) or
+                (body2 == palm_body_id and body1 == rail_body_id)):
+                palm_touching_rail = True
+                break
+        ### SPARSE reward for palm touching rail ###
+        palm_touch_rwd = 5.0 if palm_touching_rail else 0.0
+
+        ### DENSE reward for palm close to rail ###
+        rail_center_pos = self.sim.data.body_xpos[rail_body_id]
+        # Distance from palm to the rail *surface*
+        palm_to_rail_surface_dist = max(0.0, np.linalg.norm(obs_dict["palm_pos"] - rail_center_pos) - 0.277)
+        # Reward: higher when palm is closer to the surface
+        dist_reward = np.exp(-10.0 * palm_to_rail_surface_dist)
+
+        # calculate wheel rotation, just want it as big and negative as possible
         wheel_angle_now = self.sim.data.qpos[self.wheel_joint_id]
-        wheel_rotation = wheel_angle_now - self.init_wheel_angle
-        wheel_target = np.pi / 2  # 90 degrees
+        wheel_rotation = -1*(wheel_angle_now - self.init_wheel_angle) #rotate cw? I think?
 
-        wheel_rotation_err = abs(wheel_rotation - wheel_target)
-        wheel_rotation_rwd = math.exp(-5.0 * wheel_rotation_err)
-
-        
-        
+        # minimize muscle activation for realism
         act_mag = np.linalg.norm(self.obs_dict['act'], axis=-1)/self.sim.model.na if self.sim.model.na !=0 else 0
-        drop = dist_right > 0.500
 
+        # gripping, minimize distance between palm and fingertips
         fin_keys = ['fin0', 'fin1', 'fin2', 'fin3', 'fin4']
-        # for fin in fin_keys:
-        #     print(fin, type(obs_dict[fin]), np.shape(obs_dict[fin]))
         fin_open = sum(
-            np.linalg.norm(obs_dict[fin].squeeze() - obs_dict['rail_bottom_right'].squeeze(), axis=-1)
+            np.linalg.norm(obs_dict[fin].squeeze() - obs_dict['palm_pos'].squeeze(), axis=-1)
             for fin in fin_keys
         )
-        
-        # grip_right = self._check_hand_grip_contact(
-        #     hand_geom_names=["right_index_tip", "right_thumb_tip"],
-        #     wheel_geom_names=[f"handrail_coll{i}" for i in range(1, 17)]
-        # )
 
-        # Compute distance from current hand pose to initial pose
-        hand_current_pos = self.sim.data.site_xpos[self.palm_r]
-        hand_target_pos = self.sim.data.site_xpos[self.hand_start_right]
-        hand_return_err = np.linalg.norm(hand_current_pos - hand_target_pos)
+        # ELBOW JOINT VALUE
+        elbow_now = self.sim.data.qpos[self.elbow_joint_id]
+
+        #errs
+        hand_err = np.linalg.norm(obs_dict["hand_err"])
+        return_err = np.linalg.norm(obs_dict["return_err"])
 
         if return_phase:
         # Return phase: only use return_reward
-            return_reward = np.exp(-50.0 * hand_return_err) - 0.5 * hand_return_err
+            return_reward = np.exp(-50.0 * return_err) - 0.5 * return_err
             rwd_dict = collections.OrderedDict((
-                ('goal_dist', 0.0),
-                ('hand_dist', 0.0),
-                ('bonus', 0.0),
-                ('act_reg', 0.0),
-                ('fin_open', 0),
-                ('penalty', 0.0),
-                ('sparse', 0.0),
-                ('solved', False),
-                ('done', False),
-                ('wheel_rotation', 0.0),
-                ('rotation_bonus', 0.0),
-                ('return_reward', return_reward)
-                ))
-            print(f"[Return] Step: {self.time_step}, err: {hand_return_err:.3f}, reward: {return_reward:.3f}")
+                ('return_rwd', return_reward),
+                ('hand_err_rwd', 0),
+                ('dist_reward', 0.5*dist_reward),
+                ('palm_touch_rwd', 0),
+                ('wheel_rotation', 0),
+                ('act_reg', -0.25 * act_mag),
+                ('fin_open', np.exp(fin_open)),
+                ('bonus', 0),
+                ('penalty', 0),
+                ('sparse', return_err < 0.025),
+                ('solved', 0),
+                ('done', 0),
+            ))
+                # ('solved', return_err < 0.0025),
+                # ('done', return_err > 50.0),
+            print(f"[Return] Step: {self.time_step}, err: {return_err:.3f}, reward: {return_reward:.3f}")
         else:
             # Normal phase: compute all rewards
             rwd_dict = collections.OrderedDict((
-                ('goal_dist', math.exp(-2.0 * abs(dist_right))),
-                ('hand_dist', math.exp(-1.0 * abs(hand_initpos_err_right))),
-                ('bonus', 1. * (dist_right < 2 * 0) + 1. * (dist_right < 0)),
-                ('act_reg', -1. * act_mag),
-                ('fin_open', np.exp(-20 * fin_open)),
-                ('penalty', -1. * drop),
-                ('sparse', dist_right < 0.055),
-                ('solved', dist_right < 0.001 and wheel_rotation_err < 0.05),
-                ('done', dist_right > 0.9),
-                ('wheel_rotation', wheel_rotation_rwd),
-                ('rotation_bonus', 1.0 if wheel_rotation_err < 0.05 else 0.0),
-                ('return_reward', 0.0)
+                ('return_rwd', 0),
+                ('hand_err_rwd', math.exp(-20.0 * abs(hand_err))),
+                ('dist_reward', dist_reward),
+                ('palm_touch_rwd', palm_touch_rwd),
+                ('wheel_rotation', 15.0 * wheel_rotation),
+                ('act_reg', -0.5 * act_mag),
+                ('fin_open', np.exp(-5.0 * fin_open)),
+                ('bonus', 1.0 * (wheel_angle_now < self.init_wheel_angle)),
+                ('penalty', -1.0 * (wheel_angle_now > self.init_wheel_angle)),
+                ('sparse', 0),
+                ('solved', 0),
+                ('done', 0)
             ))
 
-
-        
-        rwd_dict['dense'] = np.sum([wt*rwd_dict[key] for key, wt in self.rwd_keys_wt.items()], axis=0)
-        
+        # === Compute reward based on phase ===
+        # if self.task_phase == "push":
+        #     rwd_dict = collections.OrderedDict((
+        #         ('return_rwd', 0),
+        #         ('hand_err_rwd', math.exp(-20.0 * abs(hand_err))),
+        #         ('dist_reward', dist_reward),
+        #         ('palm_touch_rwd', palm_touch_rwd),
+        #         ('wheel_rotation', 15.0 * wheel_rotation),
+        #         ('act_reg', -0.5 * act_mag),
+        #         ('fin_open', np.exp(-5.0 * fin_open)),
+        #         ('bonus', 1.0 * (wheel_angle_now < self.init_wheel_angle)),
+        #         ('penalty', -1.0 * (wheel_angle_now > self.init_wheel_angle)),
+        #         ('sparse', 0),
+        #         ('solved', 0),
+        #         ('done', 0),
+        #         # ('solved', wheel_rotation < -500.0),
+        #         # ('done', wheel_rotation > 500.0),
+        #     ))
+        #     if abs(wheel_rotation) >= (np.pi / 4):
+        #         print("Eighth rotation reached")
+        #         print("returning")
+        #         self.task_phase = "return"
+
+        # elif self.task_phase == "return":
+        #     return_reward = np.exp(-50.0 * return_err) - 0.5 * return_err
+        #     rwd_dict = collections.OrderedDict((
+        #         ('return_rwd', return_reward),
+        #         ('hand_err_rwd', 0),
+        #         ('dist_reward', 0.5*dist_reward),
+        #         ('palm_touch_rwd', 0),
+        #         ('wheel_rotation', 0),
+        #         ('act_reg', -0.25 * act_mag),
+        #         ('fin_open', np.exp(fin_open)),
+        #         ('bonus', 0),
+        #         ('penalty', 0),
+        #         ('sparse', return_err < 0.025),
+        #         ('solved', 0),
+        #         ('done', 0),
+        #         # ('solved', return_err < 0.0025),
+        #         # ('done', return_err > 50.0),
+        #         print(f"[Return] Step: {self.time_step}, err: {return_err:.3f}, reward: {return_reward:.3f}")
+        #     ))
+        #     if return_err < 0.005 and elbow_now > 1.0:
+        #         print("return successful, pushing again")
+        #         self.task_phase = "push"
+
+        # else:
+        #     raise ValueError(f"Unknown task phase: {self.task_phase}")
+
+        rwd_dict['dense'] = np.sum([wt * rwd_dict[key] for key, wt in self.rwd_keys_wt.items()])
+        # self.prev_rwd_dict = rwd_dict  # Always update
         return rwd_dict
     
     def reset(self, **kwargs):
         self.robot.sync_sims(self.sim, self.sim_obsd)
-        self.time_step = 0
         obs = super().reset(**kwargs)
+        self.time_step = 0
+        # self.prev_rwd_dict = self.get_reward_dict(self.get_obs_dict(self.sim))  # optional init
+
         return obs
-    
+
     def step(self, action):
         self.time_step += 1
-        return super().step(action)
+        return super().step(action)
\ No newline at end of file
