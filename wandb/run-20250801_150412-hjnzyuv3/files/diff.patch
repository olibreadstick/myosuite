diff --git a/myosuite/envs/myo/assets/wheelchair/myowc+arm.xml b/myosuite/envs/myo/assets/wheelchair/myowc+arm.xml
index 2ec3164..5456b0b 100644
--- a/myosuite/envs/myo/assets/wheelchair/myowc+arm.xml
+++ b/myosuite/envs/myo/assets/wheelchair/myowc+arm.xml
@@ -30,9 +30,9 @@
     </worldbody>
  
     <keyframe>
-        <key name = "new_closed" qpos="0 0 0.0031 0.999919 0.0127497 0 0 0 0 0 0 0 0 -0.465 0.1272 -0.00954 0 -0.152 0.5833 0 -0.552 -0.31314 0 -0.263915 0.40846 -0.37 2.094 1.07777 0.10997 0.4363 -0.7854 0.78 -0.6246 0.19371 0.0087015 1.17039 0.23562 0.89547 1.3432 1.46103 0.010472 0.73837 1.16254 1.33535 -0.154462 0.14139 1.571 1.24109 -0.2618 0.447735 0.59698"/>
         <key name = "start_return" qpos="0 0 0.0031 0.999919 0.0127497 0 0 0 0 0 0 0 0 -0 0.29733 -0.00954 0 -0.152 0.28244 0 -0 -0.32542 0 -0.735155 0.3142 -0.37 1.54425 0.941635 0.21994 -0.1745 -0.541926 0.78 -0.6246 0.19371 0.0087015 1.17039 0.23562 0.89547 1.3432 1.46103 0.010472 0.73837 1.16254 1.33535 -0.154462 0.14139 1.571 1.24109 -0.2618 0.447735 0.59698"/>
-    </keyframe>
+        <key name = "new_closed" qpos="0 0 0.0031 0.999919 0.0127497 0 0 0 0 0 0 0 0 -0.465 0.1272 -0.00954 0 -0.152 0.5833 0 -0.552 -0.31314 0 -0.263915 0.40846 -0.37 2.094 1.07777 0.10997 0.4363 -0.7854 0.78 -0.6246 0.19371 0.0087015 1.17039 0.23562 0.89547 1.3432 1.46103 0.010472 0.73837 1.16254 1.33535 -0.154462 0.14139 1.571 1.24109 -0.2618 0.447735 0.59698"/>
+        </keyframe>
 
     <visual>
         <scale contactwidth = "0.05" contactheight = "0.05"/>
diff --git a/myosuite/envs/myo/myowheelchair/wheelhold_v0_traj_olivia.py b/myosuite/envs/myo/myowheelchair/wheelhold_v0_traj_olivia.py
deleted file mode 100644
index 18d70f8..0000000
--- a/myosuite/envs/myo/myowheelchair/wheelhold_v0_traj_olivia.py
+++ /dev/null
@@ -1,168 +0,0 @@
-""" =================================================
-# Copyright (c) Facebook, Inc. and its affiliates
-Authors  :: Vikash Kumar (vikashplus@gmail.com), Vittorio Caggiano (caggiano@gmail.com)
-================================================= """
-
-# Trying trajectory approach
-
-import collections
-import numpy as np
-import math
-from myosuite.utils import gym
-import mujoco
-
-from myosuite.envs.myo.base_v0 import BaseV0
-
-
-class WheelHoldFixedEnvV0(BaseV0):
-
-    DEFAULT_OBS_KEYS = ['time', 'hand_qpos', 'hand_qvel']
-    DEFAULT_RWD_KEYS_AND_WEIGHTS = {
-        "trajectory_rwd": 5.0,
-        "bonus": 2.0,
-        "penalty": 5.0,
-    }
-
-    def __init__(self, model_path, obsd_model_path=None, seed=None, **kwargs):
-
-        # EzPickle.__init__(**locals()) is capturing the input dictionary of the init method of this class.
-        # In order to successfully capture all arguments we need to call gym.utils.EzPickle.__init__(**locals())
-        # at the leaf level, when we do inheritance like we do here.
-        # kwargs is needed at the top level to account for injection of __class__ keyword.
-        # Also see: https://github.com/openai/gym/pull/1497
-        gym.utils.EzPickle.__init__(self, model_path, obsd_model_path, seed, **kwargs)
-
-        # This two step construction is required for pickling to work correctly. All arguments to all __init__
-        # calls must be pickle friendly. Things like sim / sim_obsd are NOT pickle friendly. Therefore we
-        # first construct the inheritance chain, which is just __init__ calls all the way down, with env_base
-        # creating the sim / sim_obsd instances. Next we run through "setup"  which relies on sim / sim_obsd
-        # created in __init__ to complete the setup.
-        super().__init__(model_path=model_path, obsd_model_path=obsd_model_path, seed=seed, env_credits=self.MYO_CREDIT)
-
-        self._setup(**kwargs)
-
-
-    def _setup(self,
-            obs_keys:list = DEFAULT_OBS_KEYS,
-            weighted_reward_keys:list = DEFAULT_RWD_KEYS_AND_WEIGHTS,
-            **kwargs,
-        ):
-
-        self.palm_r = self.sim.model.site_name2id("palm_r")
-        self.target_qpos = self.sim.model.key_qpos[0].copy() # hand closing in on rail
-
-        self.return_traj = self.create_return_trajectory(steps=100)
-        self.return_start_time = self.sim.data.time
-        self.return_duration = 1.5  # seconds
-
-        super()._setup(obs_keys=obs_keys,
-                    weighted_reward_keys=weighted_reward_keys,
-                    **kwargs,
-        )
-        self.init_qpos = self.sim.model.key_qpos[1].copy() # copy returning keyframe
-        
-    def get_obs_vec(self):
-        self.obs_dict['time'] = np.array([self.sim.data.time])
-        self.obs_dict['hand_qpos'] = self.sim.data.qpos[13:].copy()
-        self.obs_dict['hand_qvel'] = self.sim.data.qvel[12:].copy()*self.dt
-
-        self.obs_dict["palm_pos"] = self.sim.data.site_xpos[self.palm_r]
-
-        if self.sim.model.na>0:
-            self.obs_dict['act'] = self.sim.data.act[:].copy()
-
-        t, obs = self.obsdict2obsvec(self.obs_dict, self.obs_keys)
-        return obs
-
-    def get_obs_dict(self, sim):
-        obs_dict = {}
-        obs_dict['time'] = np.array([sim.data.time])
-        obs_dict['hand_qpos'] = sim.data.qpos[13:].copy()
-        obs_dict['hand_qvel'] = sim.data.qvel[12:].copy()*self.dt
-        obs_dict["palm_pos"] = sim.data.site_xpos[self.palm_r]
-
-        if sim.model.na>0:
-            obs_dict['act'] = sim.data.act[:].copy()
-        
-        return obs_dict
-
-    def get_reward_dict(self, obs_dict):
-        step = int((self.sim.data.time - self.return_start_time) / self.return_duration * len(self.return_traj))
-        step = np.clip(step, 0, len(self.return_traj) - 1)
-        target_qpos = self.return_traj[step]
-        current_qpos = self.sim.data.qpos[13:]
-
-        traj_err = np.linalg.norm(current_qpos - target_qpos) / len(current_qpos)
-        
-        rwd_dict = collections.OrderedDict((
-            # Optional shaping reward
-            ('trajectory_rwd', -traj_err),
-
-            # MUST KEYS
-            ('bonus', 1.0 * (traj_err < 0.2) + 1.0 * (traj_err < 0.1)),
-            ('penalty', -1.0 * (traj_err > 1.5)),  # drop if far off path
-            ('sparse', -10.0 * traj_err),
-            ('solved', float(traj_err < 0.0025)),
-            ('done', bool(traj_err > 1.5)),
-        ))
-
-        
-        rwd_dict['dense'] = np.sum([wt * rwd_dict[key] for key, wt in self.rwd_keys_wt.items()])
-        # self.prev_rwd_dict = rwd_dict  # Always update
-        return rwd_dict
-    
-    def reset(self, **kwargs):
-        self.return_traj = self.create_return_trajectory(steps=50)
-        self.return_start_time = self.sim.data.time
-        self.return_duration = 1.0  # seconds
-
-        self.robot.sync_sims(self.sim, self.sim_obsd)
-        obs = super().reset(**kwargs)
-
-        self.sim.data.qpos[:] = self.sim.model.key_qpos[1]
-        self.sim.data.qvel[:] = 0
-
-        self.compute_inverse_dynamics_torques("return_torques.csv")  # Add this line if desired
-
-        return obs
-
-    def create_return_trajectory(self, steps=100):
-        start = self.sim.model.key_qpos[1][13:].copy()
-        goal = self.sim.model.key_qpos[0][13:].copy()
-        return [((1 - t) * start + t * goal) for t in np.linspace(0, 1, steps)]
-
-
-    def compute_inverse_dynamics_torques(self, save_path="return_torques.csv"):
-        """Compute inverse dynamics torques for the return trajectory."""
-        torques = []
-        traj = self.return_traj
-        n_steps = len(traj)
-
-        m = self.sim.model.ptr  # raw mujoco.MjModel pointer
-        d = self.sim.data.ptr   # raw mujoco.MjData pointer
-
-        for i in range(n_steps - 1):
-            qpos = traj[i]
-            qvel = traj[i + 1] - traj[i]
-            qacc = traj[i + 2] - 2 * traj[i + 1] + traj[i] if i + 2 < n_steps else np.zeros_like(qvel)
-
-            full_qpos = self.sim.data.qpos.copy()
-            full_qvel = self.sim.data.qvel.copy()
-            full_qpos[13:] = qpos
-            full_qvel[12:] = qvel / self.dt
-            qacc_full = np.zeros_like(full_qvel)
-            qacc_full[12:] = qacc / (self.dt ** 2)
-
-            self.sim.data.qpos[:] = full_qpos
-            self.sim.data.qvel[:] = full_qvel
-            self.sim.data.qacc[:] = qacc_full
-
-            mujoco.mj_forward(m, d)
-            mujoco.mj_inverse(m, d)
-
-            tau = self.sim.data.qfrc_inverse.copy()
-            torques.append(tau[12:])
-
-        torques = np.array(torques)
-        np.savetxt(save_path, torques, delimiter=",")
-        print(f"[INFO] Saved inverse dynamics torques to: {save_path}")
diff --git a/wandb/latest-run b/wandb/latest-run
index 2ba555c..ebc35e7 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250731_164622-79gd62cf
\ No newline at end of file
+run-20250801_150412-hjnzyuv3
\ No newline at end of file
