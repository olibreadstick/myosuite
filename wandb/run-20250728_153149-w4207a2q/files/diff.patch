diff --git a/logs/myoHandWheelHoldFixed-v0_0.monitor.csv b/logs/myoHandWheelHoldFixed-v0_0.monitor.csv
index 7eeef8a..b111c31 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_0.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_0.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_1.monitor.csv b/logs/myoHandWheelHoldFixed-v0_1.monitor.csv
index 2d8147b..a35fb6d 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_1.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_1.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_10.monitor.csv b/logs/myoHandWheelHoldFixed-v0_10.monitor.csv
index dae3f1c..1a0822e 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_10.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_10.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_11.monitor.csv b/logs/myoHandWheelHoldFixed-v0_11.monitor.csv
index 9554fc4..43620b8 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_11.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_11.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_2.monitor.csv b/logs/myoHandWheelHoldFixed-v0_2.monitor.csv
index 38ebc40..ffbfddf 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_2.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_2.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_3.monitor.csv b/logs/myoHandWheelHoldFixed-v0_3.monitor.csv
index 9b3604e..db3f9ef 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_3.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_3.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_4.monitor.csv b/logs/myoHandWheelHoldFixed-v0_4.monitor.csv
index d13997f..ee1d396 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_4.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_4.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_5.monitor.csv b/logs/myoHandWheelHoldFixed-v0_5.monitor.csv
index bc4b102..73fe313 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_5.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_5.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_6.monitor.csv b/logs/myoHandWheelHoldFixed-v0_6.monitor.csv
index f8c48ef..b918415 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_6.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_6.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_7.monitor.csv b/logs/myoHandWheelHoldFixed-v0_7.monitor.csv
index e842441..7a3bb2d 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_7.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_7.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_8.monitor.csv b/logs/myoHandWheelHoldFixed-v0_8.monitor.csv
index 54391fa..c933c03 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_8.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_8.monitor.csv differ
diff --git a/logs/myoHandWheelHoldFixed-v0_9.monitor.csv b/logs/myoHandWheelHoldFixed-v0_9.monitor.csv
index 5e4ed14..479644a 100644
Binary files a/logs/myoHandWheelHoldFixed-v0_9.monitor.csv and b/logs/myoHandWheelHoldFixed-v0_9.monitor.csv differ
diff --git a/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py b/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py
index c8e63b0..b6fa219 100644
--- a/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py
+++ b/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py
@@ -174,19 +174,19 @@ if __name__ == "__main__":
 
     print("Begin training")
 
-    model = PPO('MlpPolicy', envs, verbose=1, ent_coef=0.001,
-            policy_kwargs=policy_kwargs,
-            tensorboard_log=f"runs/{time_now}")
+    # model = PPO('MlpPolicy', envs, verbose=1, ent_coef=0.001,
+           # policy_kwargs=policy_kwargs,
+           # tensorboard_log=f"runs/{time_now}")
     
     # TODO TRY LOADING
-    # model_num =  '2025_07_24_15_16_38'
-    # model = PPO.load('./MPL_baselines/policy_best_model'+ '/'+ env_name + '/' + model_num + r'/best_model', envs, verbose = 1, ent_coeff = 0.01, policy_kwargs = policy_kwargs, tensorboard_log=f"runs/{time_now}")
+    model_num =  '2025_07_24_15_22_37_Good'
+    model = PPO.load('./MPL_baselines/policy_best_model'+ '/'+ env_name + '/' + model_num + r'/best_model', envs, verbose = 1, ent_coeff = 0.01, policy_kwargs = policy_kwargs, tensorboard_log=f"runs/{time_now}")
 
     obs_callback = TensorboardCallback()
     callback = CallbackList([eval_callback, WandbCallback(gradient_save_freq=100)])#, obs_callback])
 
     #TODO TOTAL TIMESTEPS HERE
-    model.learn(total_timesteps=1e6, tb_log_name=env_name + "_" + time_now, callback=callback)
+    model.learn(total_timesteps=5e5, tb_log_name=env_name + "_" + time_now, callback=callback)
     model.save(curr_dir+'/WheelDist_policy')
 
     # evaluate policy
diff --git a/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip b/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip
index 614b7dd..debcd1d 100644
Binary files a/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip and b/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip differ
diff --git a/myosuite/envs/myo/myowheelchair/render.py b/myosuite/envs/myo/myowheelchair/render.py
index 4e12e03..e9f185e 100644
--- a/myosuite/envs/myo/myowheelchair/render.py
+++ b/myosuite/envs/myo/myowheelchair/render.py
@@ -11,7 +11,7 @@ if __name__ == "__main__":
     env.reset()
 
     model = PPO("MlpPolicy", env, verbose=0)
-    pi = PPO.load(r"C:\Users\jasmi\Documents\GitHub\myosuite\MPL_baselines\policy_best_model\myoHandWheelHoldFixed-v0\2025_07_24_15_22_37_Good\best_model.zip")
+    pi = PPO.load(r"/Users/oliviacardillo/myosuite/myosuite3/MPL_baselines/policy_best_model/myoHandWheelHoldFixed-v0/2025_07_28_15_14_23/best_model.zip")
 
     # render
     frames = []
diff --git a/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4 b/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4
index 028cd3b..c75cadf 100644
Binary files a/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4 and b/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4 differ
diff --git a/myosuite/envs/myo/myowheelchair/wheelhold_v0.py b/myosuite/envs/myo/myowheelchair/wheelhold_v0.py
index 0dd7bf9..88be7c7 100644
--- a/myosuite/envs/myo/myowheelchair/wheelhold_v0.py
+++ b/myosuite/envs/myo/myowheelchair/wheelhold_v0.py
@@ -43,6 +43,10 @@ class WheelHoldFixedEnvV0(BaseV0):
         # created in __init__ to complete the setup.
         super().__init__(model_path=model_path, obsd_model_path=obsd_model_path, seed=seed, env_credits=self.MYO_CREDIT)
 
+        self.init_wheel_angle = None
+
+
+
         self._setup(**kwargs)
 
 
@@ -51,6 +55,9 @@ class WheelHoldFixedEnvV0(BaseV0):
             weighted_reward_keys:list = DEFAULT_RWD_KEYS_AND_WEIGHTS,
             **kwargs,
         ):
+        self.time_step = 0
+        self.return_phase_start = 40
+        
         # self.goal_sid_right = self.sim.model.site_name2id("wheelchair_grip_right") #green, movable
         self.palm_r = self.sim.model.site_name2id("palm_r")
         self.hand_start_right = self.sim.model.site_name2id("hand_start_right") #red, static
@@ -69,15 +76,13 @@ class WheelHoldFixedEnvV0(BaseV0):
         self.fin4 = self.sim.model.site_name2id("LFtip")
 
         self.wheel_joint_id = self.sim.model.joint_name2id("right_rear") #right wheel joint
-        self.init_wheel_angle = self.sim.data.qpos[self.wheel_joint_id].copy() #INIITAL wheel angle
+        #self.init_wheel_angle = self.sim.data.qpos[self.wheel_joint_id].copy() #INIITAL wheel angle
 
         #elbow jnt
         self.elbow_joint_id = self.sim.model.joint_name2id("elbow_flexion")
 
         #phases check
         self.task_phase = "push"
-        self.triggered_return_bonus = False
-        self.triggered_push_bonus = False
         # self.prev_rwd_dict = None
 
         super()._setup(obs_keys=obs_keys,
@@ -106,7 +111,7 @@ class WheelHoldFixedEnvV0(BaseV0):
         # self.obs_dict["rail_bottom_right"] = self.sim.data.site_xpos[self.rail_bottom_right]
 
         self.obs_dict['wheel_angle'] = np.array([self.sim.data.qpos[self.wheel_joint_id]])
-        self.obs_dict["task_phase"] = np.array([1.0 if self.task_phase == "push" else -1.0])
+        # self.obs_dict["task_phase"] = np.array([1.0 if self.task_phase == "push" else -1.0])
 
         #calculate palm from target distance
         self.obs_dict['hand_err'] = self.sim.data.site_xpos[self.palm_r]- self.sim.data.site_xpos[self.hand_TARGET_right]
@@ -139,7 +144,7 @@ class WheelHoldFixedEnvV0(BaseV0):
         # obs_dict["rail_bottom_right"] = sim.data.site_xpos[self.rail_bottom_right]
         obs_dict['wheel_angle'] = np.array([sim.data.qpos[self.wheel_joint_id]])
 
-        obs_dict["task_phase"] = np.array([1.0 if self.task_phase == "push" else -1.0])
+        # obs_dict["task_phase"] = np.array([1.0 if self.task_phase == "push" else -1.0])
 
         #calculate palm from target distance
         obs_dict['hand_err'] = self.sim.data.site_xpos[self.palm_r]- self.sim.data.site_xpos[self.hand_TARGET_right]
@@ -153,6 +158,9 @@ class WheelHoldFixedEnvV0(BaseV0):
         return obs_dict
 
     def get_reward_dict(self, obs_dict):
+        return_phase = self.time_step >= self.return_phase_start
+        
+        
         # palm position from green dot (WARNING: MOVES WITH RAIL)
         # dist_right = np.linalg.norm(obs_dict['wheel_err_right']) 
 
@@ -180,8 +188,16 @@ class WheelHoldFixedEnvV0(BaseV0):
 
         # calculate wheel rotation, just want it as big and negative as possible
         wheel_angle_now = self.sim.data.qpos[self.wheel_joint_id]
+        
+        if self.init_wheel_angle is None:
+            self.init_wheel_angle = wheel_angle_now.copy()
+            print(f"[Fallback] init_wheel_angle was unset, now set to {self.init_wheel_angle:.3f}")
+
+        
         wheel_rotation = -1*(wheel_angle_now - self.init_wheel_angle) #rotate cw? I think?
 
+    
+
         # minimize muscle activation for realism
         act_mag = np.linalg.norm(self.obs_dict['act'], axis=-1)/self.sim.model.na if self.sim.model.na !=0 else 0
 
@@ -199,7 +215,44 @@ class WheelHoldFixedEnvV0(BaseV0):
         hand_err = np.linalg.norm(obs_dict["hand_err"])
         return_err = np.linalg.norm(obs_dict["return_err"])
 
-        # === Compute reward based on phase ===
+        # if return_phase:
+        # # Return phase: only use return_reward
+        #     return_reward = np.exp(-50.0 * return_err) - 0.5 * return_err
+        #     rwd_dict = collections.OrderedDict((
+        #         ('return_rwd', return_reward),
+        #         ('hand_err_rwd', 0),
+        #         ('dist_reward', 0.5*dist_reward),
+        #         ('palm_touch_rwd', 0),
+        #         ('wheel_rotation', 0),
+        #         ('act_reg', -0.25 * act_mag),
+        #         ('fin_open', np.exp(fin_open)),
+        #         ('bonus', 0),
+        #         ('penalty', 0),
+        #         ('sparse', return_err < 0.025),
+        #         ('solved', 0),
+        #         ('done', 0),
+        #     ))
+        #         # ('solved', return_err < 0.0025),
+        #         # ('done', return_err > 50.0),
+        #     print(f"[Return] Step: {self.time_step}, err: {return_err:.3f}, reward: {return_reward:.3f}")
+        # else:
+        #     # Normal phase: compute all rewards
+        #     rwd_dict = collections.OrderedDict((
+        #         ('return_rwd', 0),
+        #         ('hand_err_rwd', math.exp(-20.0 * abs(hand_err))),
+        #         ('dist_reward', dist_reward),
+        #         ('palm_touch_rwd', palm_touch_rwd),
+        #         ('wheel_rotation', 15.0 * wheel_rotation),
+        #         ('act_reg', -0.5 * act_mag),
+        #         ('fin_open', np.exp(-5.0 * fin_open)),
+        #         ('bonus', 1.0 * (wheel_angle_now < self.init_wheel_angle)),
+        #         ('penalty', -1.0 * (wheel_angle_now > self.init_wheel_angle)),
+        #         ('sparse', 0),
+        #         ('solved', 0),
+        #         ('done', 0)
+        #     ))
+
+    
         if self.task_phase == "push":
             print(f"[Push Phase] hand_err: {hand_err:.4f}, elbow: {elbow_now:.4f}, wheel: {wheel_rotation:.4f}")
 
@@ -223,6 +276,12 @@ class WheelHoldFixedEnvV0(BaseV0):
                 # ('done', wheel_rotation > 500.0),
             ))
             
+            # print(f"[Push] Step: init = {self.init_wheel_angle:.3f}, now = {wheel_angle_now:.3f}, rotation = {wheel_rotation:.3f}")
+            # if abs(wheel_rotation) >= abs(np.pi / 18):
+            #     print("10 deg reached")
+            #     print("returning")
+            #     self.task_phase = "return"
+            
             if triggered_return and not self.triggered_return_bonus:
                 print("returning")
                 self.task_phase = "return"
@@ -266,7 +325,13 @@ class WheelHoldFixedEnvV0(BaseV0):
     def reset(self, **kwargs):
         self.robot.sync_sims(self.sim, self.sim_obsd)
         obs = super().reset(**kwargs)
-
+        self.time_step = 0
         # self.prev_rwd_dict = self.get_reward_dict(self.get_obs_dict(self.sim))  # optional init
-        
-        return obs
\ No newline at end of file
+
+        self.init_wheel_angle = self.sim.data.qpos[self.wheel_joint_id].copy()
+
+        return obs
+
+    def step(self, action):
+        self.time_step += 1
+        return super().step(action)
\ No newline at end of file
