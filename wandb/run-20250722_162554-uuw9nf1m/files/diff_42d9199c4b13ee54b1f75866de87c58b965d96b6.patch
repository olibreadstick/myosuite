diff --git a/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py b/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py
index 1991137..8004188 100644
--- a/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py
+++ b/myosuite/envs/myo/myowheelchair/ParallelTrainingScriptWandB.py
@@ -163,11 +163,11 @@ if __name__ == "__main__":
 
     print("Begin training")
 
-    # model = PPO('MlpPolicy', envs, verbose=1, ent_coef=0.001, policy_kwargs=policy_kwargs)
+    model = PPO('MlpPolicy', envs, verbose=1, ent_coef=0.001, policy_kwargs=policy_kwargs)
 
-    # TODO TRY LOADING
-    model_num =   '2025_07_21_17_13_37'
-    model = PPO.load('./MPL_baselines/policy_best_model'+ '/'+ env_name + '/' + model_num + r'/best_model', envs, verbose = 1, ent_coeff = 0.01, policy_kwargs = policy_kwargs)
+    # # TODO TRY LOADING
+    # model_num =   '2025_07_22_13_58_57'
+    # model = PPO.load('./MPL_baselines/policy_best_model'+ '/'+ env_name + '/' + model_num + r'/best_model', envs, verbose = 1, ent_coeff = 0.01, policy_kwargs = policy_kwargs)
 
     obs_callback = TensorboardCallback()
     callback = CallbackList([eval_callback, WandbCallback(gradient_save_freq=100)])#, obs_callback])
diff --git a/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip b/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip
index 6f959af..6b10d47 100644
Binary files a/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip and b/myosuite/envs/myo/myowheelchair/WheelDist_policy.zip differ
diff --git a/myosuite/envs/myo/myowheelchair/render.py b/myosuite/envs/myo/myowheelchair/render.py
index ee5efc3..736c002 100644
--- a/myosuite/envs/myo/myowheelchair/render.py
+++ b/myosuite/envs/myo/myowheelchair/render.py
@@ -11,7 +11,7 @@ if __name__ == "__main__":
     env.reset()
 
     model = PPO("MlpPolicy", env, verbose=0)
-    pi = PPO.load(r"C:\Users\jasmi\Documents\GitHub\myosuite\MPL_baselines\policy_best_model\myoHandWheelHoldFixed-v0\2025_07_21_17_58_43\best_model.zip")
+    pi = PPO.load(r"C:\Users\jasmi\Documents\GitHub\myosuite\myosuite\envs\myo\myowheelchair\WithRot_good_grip.zip")
 
     env = gym.make('myoHandWheelHoldFixed-v0')
     env.reset()
@@ -19,7 +19,7 @@ if __name__ == "__main__":
     # render
     frames = []
     for _ in range(300):
-        frames.append(env.sim.renderer.render_offscreen(width=400, height=400, camera_id=0)) 
+        frames.append(env.sim.renderer.render_offscreen(width=400, height=400, camera_id=5)) 
         o = env.get_obs()
         a = pi.predict(o)[0]
         next_o, r, done, *_, ifo = env.step(
diff --git a/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4 b/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4
index ec9bf35..c79804d 100644
Binary files a/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4 and b/myosuite/envs/myo/myowheelchair/videos/HandFocusedRender.mp4 differ
diff --git a/myosuite/envs/myo/myowheelchair/videos/Wheel_MinDist.mp4 b/myosuite/envs/myo/myowheelchair/videos/Wheel_MinDist.mp4
index f0a1d0b..aa77a5d 100644
Binary files a/myosuite/envs/myo/myowheelchair/videos/Wheel_MinDist.mp4 and b/myosuite/envs/myo/myowheelchair/videos/Wheel_MinDist.mp4 differ
diff --git a/myosuite/envs/myo/myowheelchair/wheelhold_v0.py b/myosuite/envs/myo/myowheelchair/wheelhold_v0.py
index 8aaebcb..dd9953d 100644
--- a/myosuite/envs/myo/myowheelchair/wheelhold_v0.py
+++ b/myosuite/envs/myo/myowheelchair/wheelhold_v0.py
@@ -3,10 +3,6 @@
 Authors  :: Vikash Kumar (vikashplus@gmail.com), Vittorio Caggiano (caggiano@gmail.com)
 ================================================= """
 
-## PUSHING WHEELCHAIR ##
-
-# TODO traing from 174109, make fin open 0, make wheel rotation 15???
-
 import collections
 import numpy as np
 import math
@@ -17,15 +13,15 @@ from myosuite.envs.myo.base_v0 import BaseV0
 
 class WheelHoldFixedEnvV0(BaseV0):
 
-    DEFAULT_OBS_KEYS = ['time', 'wheel_err_right', 'wheel_angle', 'hand_qpos', 'hand_qvel']
+    DEFAULT_OBS_KEYS = ['time', 'wheel_err_right', 'wheel_angle', 'hand_qpos', 'hand_qvel', 'task_phase']
     DEFAULT_RWD_KEYS_AND_WEIGHTS = {
-        "goal_dist": 0.0,
-        "hand_dist" : 0.0,
-        "fin_open": -10.0,
+        "goal_dist": 15.0,
+        "hand_dist" : 5.0,
+        "fin_open": 15.0,
         "bonus": 0.0,
-        "penalty": 0,
-        "wheel_rotation": 10.0,
-        "rotation_bonus": 2.0
+        "penalty": 2,
+        "wheel_rotation": 0.0,
+        "rotation_bonus": 0.0
     }
 
     def __init__(self, model_path, obsd_model_path=None, seed=None, **kwargs):
@@ -49,7 +45,7 @@ class WheelHoldFixedEnvV0(BaseV0):
 
     def _setup(self,
             obs_keys:list = DEFAULT_OBS_KEYS,
-            weighted_reward_keys:list = DEFAULT_RWD_KEYS_AND_WEIGHTS,
+            weighted_reward_keys: dict = None,  # unused now
             **kwargs,
         ):
         self.goal_sid_right = self.sim.model.site_name2id("wheelchair_grip_right")
@@ -69,15 +65,36 @@ class WheelHoldFixedEnvV0(BaseV0):
         self.wheel_joint_id = self.sim.model.joint_name2id("right_rear")
         self.init_wheel_angle = self.sim.data.qpos[self.wheel_joint_id]
 
+        self.task_phase = "push"
+        self.push_threshold = np.deg2rad(30)  # ~0.52 radians
+        self.return_threshold = 0.03  # distance in meters
+
+        self.initial_hand_pos = self.sim.data.site_xpos[self.palm_r].copy()
+        self.initial_wheel_angle = self.sim.data.qpos[self.wheel_joint_id].copy()
+
+        self.rwd_weights_push = {
+            "goal_dist": 15.0,
+            "hand_dist": 5.0,
+            "fin_open": 15.0,
+            "penalty": 2.0,
+            "wheel_rotation": 5.0,
+            "rotation_bonus": 2.0,
+        }
+
+        self.rwd_weights_return = {
+            "hand_dist": 20.0,        # reward for going back to initial hand pos
+            "fin_open": 5.0,
+            "penalty": 2.0,
+        }
 
         #self.goal_sid_left = self.sim.model.site_name2id("wheel_grip_goal_left")
         #self.object_init_pos = self.sim.data.site_xpos[self.object_sid].copy()
 
-        super()._setup(obs_keys=obs_keys,
-                    weighted_reward_keys=weighted_reward_keys,
-                    **kwargs,
-        )
-        
+        super()._setup(
+            obs_keys=obs_keys,
+            weighted_reward_keys={},  # <- pass dummy dict
+            **kwargs
+        )        
         self.init_qpos = self.sim.model.key_qpos[0].copy() # copy the sitting + grabbing wheels keyframe
 
 
@@ -100,6 +117,7 @@ class WheelHoldFixedEnvV0(BaseV0):
 
         self.obs_dict['wheel_angle'] = np.array([self.sim.data.qpos[self.wheel_joint_id]])
 
+        self.obs_dict["task_phase"] = np.array([1.0 if self.task_phase == "push" else -1.0])
 
         if self.sim.model.na>0:
             self.obs_dict['act'] = self.sim.data.act[:].copy()
@@ -108,6 +126,7 @@ class WheelHoldFixedEnvV0(BaseV0):
         return obs
 
     def get_obs_dict(self, sim):
+        
         obs_dict = {}
         obs_dict['time'] = np.array([sim.data.time])
         obs_dict['hand_qpos'] = sim.data.qpos[13:].copy()
@@ -129,6 +148,7 @@ class WheelHoldFixedEnvV0(BaseV0):
 
         obs_dict['wheel_angle'] = np.array([sim.data.qpos[self.wheel_joint_id]])
 
+        obs_dict["task_phase"] = np.array([1.0 if self.task_phase == "push" else -1.0])
 
         #obs_dict['wheel_err_left'] = sim.data.site_xpos[self.goal_sid] - sim.data.site_xpos[self.object_sid]
         if sim.model.na>0:
@@ -137,57 +157,73 @@ class WheelHoldFixedEnvV0(BaseV0):
         return obs_dict
 
     def get_reward_dict(self, obs_dict):
+        # --- Positions and angles ---
         dist_right = np.linalg.norm(obs_dict['wheel_err_right'])
-        hand_initpos_err_right = np.linalg.norm(obs_dict['hand_initpos_err_right'])
-
-        #for wheel rotation
-        wheel_angle_now = self.sim.data.qpos[self.wheel_joint_id]
-        wheel_rotation = wheel_angle_now - self.init_wheel_angle
-        wheel_target = np.pi / 2  # 90 degrees
+        palm_pos = obs_dict['palm_pos']
+        hand_return_dist = np.linalg.norm(palm_pos - self.initial_hand_pos)
 
-        wheel_rotation_err = abs(wheel_rotation - wheel_target)
+        current_wheel_angle = self.sim.data.qpos[self.wheel_joint_id]
+        wheel_rotation = current_wheel_angle - self.initial_wheel_angle
+        wheel_rotation_target = np.pi / 2
+        wheel_rotation_err = abs(wheel_rotation - wheel_rotation_target)
         wheel_rotation_rwd = math.exp(-5.0 * wheel_rotation_err)
 
-        
-        
-        act_mag = np.linalg.norm(self.obs_dict['act'], axis=-1)/self.sim.model.na if self.sim.model.na !=0 else 0
-        drop = dist_right > 0.500
+        drop = dist_right > 0.5
+        act_mag = np.linalg.norm(self.obs_dict['act']) / self.sim.model.na if self.sim.model.na != 0 else 0
 
+        # --- Finger openness ---
         fin_keys = ['fin0', 'fin1', 'fin2', 'fin3', 'fin4']
-        # for fin in fin_keys:
-        #     print(fin, type(obs_dict[fin]), np.shape(obs_dict[fin]))
         fin_open = sum(
-            np.linalg.norm(obs_dict[fin].squeeze() - obs_dict['rail_bottom_right'].squeeze(), axis=-1)
+            np.linalg.norm(obs_dict[fin] - obs_dict['rail_bottom_right'])
             for fin in fin_keys
         )
-        
-        # grip_right = self._check_hand_grip_contact(
-        #     hand_geom_names=["right_index_tip", "right_thumb_tip"],
-        #     wheel_geom_names=[f"handrail_coll{i}" for i in range(1, 17)]
-        # )
-
-        rwd_dict = collections.OrderedDict((
-            ('goal_dist', math.exp(-2.0*abs(dist_right))), #exp(- k * abs(x))
-            ('hand_dist', math.exp(-1.0*abs(hand_initpos_err_right))),
-            ('bonus', 1.*(dist_right<2*0) + 1.*(dist_right<0)),
-            ('act_reg', -1.*act_mag),
-            ("fin_open", np.exp(-20 * fin_open)),  # fin_open + np.log(fin_open +1e-8)
-
-            #('grip_bonus', 1.0 * grip_right),
-            ('penalty', -1.*drop),
-            ('sparse', dist_right < 0.055),
-            #('sparse', 1.0 * grip_right - dist_right),
-            ('solved', dist_right < 0.001 and wheel_rotation_err < 0.05),
-            #('solved', grip_right and dist_right < 0.015),
-            ('done', dist_right > 0.9),
-            ('wheel_rotation', wheel_rotation_rwd),
-            ('rotation_bonus', 1.0 if wheel_rotation_err < 0.05 else 0.0),
-        ))
-        
-        rwd_dict['dense'] = np.sum([wt*rwd_dict[key] for key, wt in self.rwd_keys_wt.items()], axis=0)
-        
+
+        # --- Phase switching ---
+        if self.task_phase == "push" and wheel_rotation > self.push_threshold:
+            self.task_phase = "return"
+        elif self.task_phase == "return" and hand_return_dist < self.return_threshold:
+            self.task_phase = "push"
+            self.initial_wheel_angle = current_wheel_angle
+
+        # --- Rewards shared across phases ---
+        goal_dist_rwd = math.exp(-2.0 * dist_right)
+        hand_dist_rwd = math.exp(-1.0 * hand_return_dist)
+        fin_open_rwd = math.exp(-20.0 * fin_open)
+        act_reg = -1.0 * act_mag
+        penalty = -1.0 * drop
+        bonus = 1.0 * (dist_right < 0.1) + 1.0 * (dist_right < 0.05)  # can tweak
+        solved = dist_right < 0.001 and wheel_rotation_err < 0.05
+        done = dist_right > 0.9
+
+        # --- Sparse reward ---
+        sparse = float(dist_right < 0.055) if self.task_phase == "push" else float(hand_return_dist < self.return_threshold)
+
+        # --- Assemble reward dict ---
+        rwd_dict = collections.OrderedDict()
+        rwd_dict["goal_dist"] = goal_dist_rwd
+        rwd_dict["hand_dist"] = hand_dist_rwd
+        rwd_dict["fin_open"] = fin_open_rwd
+        rwd_dict["bonus"] = bonus
+        rwd_dict["act_reg"] = act_reg
+        rwd_dict["penalty"] = penalty
+        rwd_dict["wheel_rotation"] = wheel_rotation_rwd
+        rwd_dict["rotation_bonus"] = 1.0 if wheel_rotation_err < 0.05 else 0.0
+        rwd_dict["sparse"] = sparse
+        rwd_dict["solved"] = solved
+        rwd_dict["done"] = done
+
+        # --- Use phase-specific weights ---
+        weights = self.rwd_weights_push if self.task_phase == "push" else self.rwd_weights_return
+        rwd_dict["dense"] = sum(weights.get(k, 0.0) * rwd_dict[k] for k in rwd_dict)
+
+        # --- Debug info ---
+        rwd_dict["phase"] = 1.0 if self.task_phase == "push" else -1.0
+        rwd_dict["wheel_angle"] = current_wheel_angle
+        rwd_dict["hand_return_dist"] = hand_return_dist
+
         return rwd_dict
-    
+
+
     def reset(self, **kwargs):
         self.robot.sync_sims(self.sim, self.sim_obsd)
         obs = super().reset(**kwargs)
